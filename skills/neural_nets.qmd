---
title: Neural Networks made ridiculously simple
date: "2024-08-12"
execute: 
  echo: false
  output: true
  freeze: false
categories: [AI/ML, beginner]
---

```{r}
#| results: asis
library(readr)
library(dplyr)
library(glue)
library(ggplot2)
library(sigmoid)

source(here::here("R/feed_block.R"))
feed_block("Neural Networks made ridiculously simple")

```

:::{.callout-note collapse=false appearance='default' icon=true}
## Session materials
+ [all materials {{< iconify ph:file-zip size=2x >}}](src/neural_nets.zip)
+ slides [{{< iconify ph:file-html size=2x >}} html](src/neural_nets.html) / [{{< iconify ph:file-pdf size=2x >}} pdf](src/neural_nets.pdf)
:::

## Welcome
* this session is ðŸŒ¶: for beginners

## What's this session for?

-   neural nets are a core technology for AI/ML systems
-   they've been around for decades (and probably will go on for decades)
-   they're also particularly helpful for health & care folk as a way of understanding AI/ML tools in general

## What this session won't do

-   give a general introduction to AI/ML
-   explain how to build a neural net of your very own
-   discuss in any detail the (often formidable) maths of neural nets

## Biology: the neurone

![[WikiMedia](https://commons.wikimedia.org/wiki/Category:Camillo_Golgi#/media/File:Golgi_1885_Plate_XIII.JPG)](images/clipboard-3856000381.png){height="550px"}

-   [biological neurones](https://en.wikipedia.org/wiki/Neuron):
    -   receive some input from an upstream neuron(s)
    -   process that input in some way
    -   generate some output(s) in response to that input, and pass it downstream

## Biology: activation

![https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Action_potential.svg/778px-Action_potential.svg.png](images/clipboard-3431060444.png){height="550px"}

-   neurones respond to stimulii
    -   threshold-y
    -   approximately digital output (on/off)
    -   sometimes complex behaviour about inputs

## Biology: networks of neurones

![[wikimedia](https://commons.wikimedia.org/wiki/Category:Camillo_Golgi#/media/File:Camillo_Golgi's_image_of_a_dog%E2%80%99s_olfactory_bulb\_(detail_2).jpg)](images/clipboard-2953597145.png){height="550px"}

-   neurones are usually found in networks
-   can produce complex and sophisticated behaviours in a robust way </br> ![[Leon Benjamin on Flickr](https://www.flickr.com/photos/lbphotos/128557218)](images/clipboard-2825173395.png)
-   non-obvious relationships between structure and function

## Machines: the node

Here's a simple representation of a node, implemented in code, that we might find in a neural network:

```{r}
#| fig-height: 3

one_node <- c('Input', 'Output')

DiagrammeR::grViz("digraph G2 {
  graph [layout=neato overlap = true]

  n2 [pos='0.5,1!' label = '@@1' shape = plaintext fontcolor=SlateGrey]
  n1 [pos='2.5,1!' label='Node' style=radial fontcolor=SlateGrey fillcolor = mistyrose color=white]
  n3 [pos='4.5,1!' label = '@@2' shape = plaintext fontcolor=SlateGrey]

  n2 -> n1 [color = SlateGrey]
  n1 -> n3 [color = SlateGrey]

}

[1]: one_node[1]
[2]: one_node[2]

")


```

## Machines: activation functions

Here are some example input:output pairs for our node:

```{r}
#| fig-height: 0.75

one_node <- c("Input = F", "Output = T")
DiagrammeR::grViz("digraph G2 {
  graph [layout=neato overlap = true]

  n2 [pos='0.5,1!' label = '@@1' shape = plaintext fontcolor=SlateGrey]
  n1 [pos='2.5,1!' label='Node' style=radial fontcolor=SlateGrey fillcolor = mistyrose color=white]
  n3 [pos='4.5,1!' label = '@@2' shape = plaintext fontcolor=SlateGrey]

  n2 -> n1 [color = SlateGrey]
  n1 -> n3 [color = SlateGrey]

}

[1]: one_node[1]
[2]: one_node[2]

")

cat("  \n")

one_node <- c("Input = T", "Output = F")
DiagrammeR::grViz("digraph G2 {
  graph [layout=neato overlap = true]

  n2 [pos='0.5,1!' label = '@@1' shape = plaintext fontcolor=SlateGrey]
  n1 [pos='2.5,1!' label='Node' style=radial fontcolor=SlateGrey fillcolor = mistyrose color=white]
  n3 [pos='4.5,1!' label = '@@2' shape = plaintext fontcolor=SlateGrey]

  n2 -> n1 [color = SlateGrey]
  n1 -> n3 [color = SlateGrey]

}

[1]: one_node[1]
[2]: one_node[2]

")

```

-   there are lots of possible activation functions
-   a simple one: `NOT`
    -   our node outputs TRUE when we input FALSE, and vice versa

This flexibility means that we can build networks of nodes (hence neural networks). Again, a very simple example:

## Activation functions can be extremely simple

```{r}
#| echo: true

node <- function(input){
  !input
}

node(TRUE)
```

## Machines: networks of nodes

```{r}
DiagrammeR::grViz("digraph G1 {
  graph [layout=neato overlap = true]     
  I0 [pos='0.5,3.25!' shape=plaintext label='Input layer' fontsize=20 fontcolor=SlateGrey]
  I1 [pos='1,1!'  style=radial fillcolor = mistyrose color=white fontcolor=SlateGrey]  
  I7 [pos='0,1!'  shape=plaintext label='Input' fontcolor=SlateGrey]
  H0 [pos='3,3.25!' shape=plaintext label='Hidden layer' fontsize=20 fontcolor=SlateGrey]
  H1 [pos='3,2.5!' style=radial fillcolor = azure color=white fontcolor=SlateGrey]     
  H2 [pos='3,1!'    style=radial fillcolor = azure color=white fontcolor=SlateGrey]
  H3 [pos='3,-0.5!' style=radial fillcolor = azure color=white fontcolor=SlateGrey]
  O0 [pos='5.5,3.25!' shape=plaintext label='Output layer' fontsize=20 fontcolor=SlateGrey]
  O1 [pos='5,1!'  style=radial  fillcolor = honeydew color=white fontcolor=SlateGrey]
  O7 [pos='6,1!' shape=plaintext label='Output' fontcolor=SlateGrey]

  I7 -> I1 [color = SlateGrey]
  I1 -> H1 [color = SlateGrey]
  I1 -> {H2 H3} [color = SlateGrey]
  {H1 H2 H3} -> O1 [color = SlateGrey]
  O1 -> O7 [color = SlateGrey]
  
}")
```

## Machines: networks of nodes

-   nodes are usually found in networks
-   can produce complex and sophisticated behaviours in a robust way </br> ![Chat GPT nonsense](images/clipboard-1429075449.png){height="400px"}
-   again, non-obvious relationships between structure and function in artifical neural networks (ANN)


::: {.notes}
A user supplies some input. That input is fed into an input node(s), which processes the input, and produces three different outputs that are then fed into a second layer of nodes. Further processing happens in this hidden layer, leading to three outputs that are integrated together in a final output node that processes the outputs of the hidden layer into a single output.
:::


## Several kinds of networks

+ [there are lots of ways that neural networks can be arranged](https://www.asimovinstitute.org/neural-network-zoo/)
+ our example above = feed-forward
  + all the nodes are connected from left-to-right
+ but more complex architectures - like [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) - might have feedback looks and other biological-ish features
+ different numbers of layers
+ lots of different design tendencies since the first intro of neural nets in the 1950s [@rosenblatt1958]
+ most fancy ANNs are currently architecturally simple

## Why ANNs?
+ ANNs can can potentially replicate *any* input-output ransformation
+ we do that by a) increasing complexity and b) allowing them to 'learn'

. . .

```{r}
#| fig-cap: 'A more complex feed-forward neural network'

DiagrammeR::grViz("digraph G2 {
  graph [rankdir = LR splines=false] 

  
  {node [style=radial fontcolor=SlateGrey fillcolor = mistyrose color=white]
  I1; I2; I3; I4; I5} 
  
  {node [style=radial fontcolor=SlateGrey fillcolor = azure color=white];
  H1a, H1b, H1c, H1d, H1e} 
  
  {node [style=radial fontcolor=SlateGrey fillcolor = azure color=white];
  H2a, H2b, H2c, H2d, H2e} 
  
  {node [style=radial fontcolor=SlateGrey fillcolor = honeydew color=white];
  O1, O2, O3, O4, O5} 
  
  l0 [shape=plaintext, label='Input layer' fontcolor = SlateGrey];
  l1 [shape=plaintext, label='Hidden layer 1' fontcolor = SlateGrey];
  l2 [shape=plaintext, label='Hidden layer 2' fontcolor = SlateGrey];
  l3 [shape=plaintext, label='Output layer' fontcolor = SlateGrey];
  
  {rank=same; l0; I1}
  {rank=same; l1; H1a}
  {rank=same; l2; H2a}
  {rank=same; l3; O1};
  
  {
  rank=same;
  I1->I2->I3->I4->I5 [color=white];
  }
  {
  rank=same;
  H1a->H1b->H1c->H1d->H1e [color=white];
  }
  {
  rank=same;
  H2a->H2b->H2c->H2d->H2e [color=white];
  }
  {
  rank=same;
  O1->O2->O3->O4->O5 [color=white];
  }
  
  {I1, I2, I3, I4, I5} -> {H1a, H1b, H1c, H1d, H1e} [color = SlateGrey arrowsize=0.3]
  {H1a, H1b, H1c, H1d, H1e} -> {H2a, H2b, H2c, H2d, H2e} [color = SlateGrey arrowsize=0.3]
  {H2a, H2b, H2c, H2d, H2e} -> {O1, O2, O3, O4, O5} [color = SlateGrey arrowsize=0.3]
}")
```


## Different activation functions

+ binary (true/false)
+ continuous
  + linear
  + non-linear (like sigmoid, ReLU)


```{r}
ggplot() +
  xlim(c(-15, 15)) +
  geom_function(fun = sigmoid) +
  theme_minimal() +
  ggtitle("Sigmoid")
```


```{r}
ggplot() +
  xlim(c(-15, 15)) +
  geom_function(fun = relu) +
  theme_minimal() +
  ggtitle("ReLU")
```

## Training in neural networks

+ ANNs can be trained
  + take a dataset 
  + split it into training and test parts
    + classify (by hand) the training data
  + then train
    + feed your ANN the training data and evaluate how well it performs
    + modify the ANN based on that evaluation
    + repeat until done/bored/perfect
  + finally, test your model with your unlabelled test data and evaluate
  
## MNIST

+ a [classic dataset](https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png) </br>![](images/MnistExamplesModified.png)
+ recognizing handwritten numbers = actually-important task
+ 60000 labelled training images
+ 10000 test images
+ each is a 28\*28 pixel matrix grey vales encoded as 0-255

## MNIST data example

```{r}
mnist <- readRDS("data/mnist_sm.rds")
t(matrix(mnist$train$images[1,], 28, 28)) |>
  as_tibble() |>
  select(V10:V20) |>
  slice(5:10) |>
  knitr::kable()
```

## Train for MNIST

+ take your training data
+ put together a neural network (number of nodes, layers, feedback, activation functions)
+ run the training data, and evaluate based on labelling
+ modify your neural network, rinse, and repeat
+ ...
+ when happy, try the unlabelled test data

## MNIST examples

+ [lots of different examples](https://www.google.com/search?sca_esv=2570ae1cf58fa5c0&sca_upv=1&sxsrf=ADLYWIKxXMZkhLseWyic66zzdMajQ_FY3Q:1723463086461&q=mnist+neural+network&tbm=isch&source=lnms&fbs=AEQNm0Aa4sjWe7Rqy32pFwRj0UkWERaHdBms7t-tHL1116ec0FnDIxrxgGhNFSZEtYqV91QqM6LWLrLFWKmjC_P6yIDKAfdUXLK5TdVXK9XKaLJ0CvIevHNUoTv_L_edtCkFO49GwFOyIj9HDXPzAaIsMKrRU9-We_G08qTzCW2H2tx2SXc4OexQzDRMByEUFhbW4wpPcER3&sa=X&ved=2ahUKEwiP-LSosO-HAxWpT0EAHdVLOBsQ0pQJegQIERAB&biw=1842&bih=1007&dpr=1)
+ why do this work in an ANN, rather than in some other tool?
  + generic training strategy - reduces need for domain knowledge
  + hopefully robust outcomes - so giving models able to work across contexts
  + scalable

## MNIST in R

An aside here for the R enthusiasts - we can plot the handwritten numbers back out of the data using `ggplot()`:

```{r}
#| warning: false
#| message: false
#| cache: true
#| echo: true
#| output-location: slide

mnist_plot_dat <- function(df) {
   # matrix to pivoted tibble for plotting
  df |>
      as_tibble() |>
      mutate(rn = row_number()) |>
      tidyr::pivot_longer(!rn) |>
      mutate(name = as.numeric(gsub("V", "", name)))
}

mnist_main_plot <- function(df) {
  df |>
    ggplot() +
    geom_tile(aes(
      x = rn,
      y = reorder(name,-name),
      fill = value
    )) +
    scale_fill_gradient2(mid = "white", high = "black")
}

mnist_plot <- function(n){

  mnist_plot_dat(matrix(mnist$train$images[n, ], 28, 28)) |>
    mnist_main_plot() +
      ggtitle(glue("Label: { mnist$train$labels[n]}")) +
      theme_void() +
      theme(legend.position = "none")

}

gridExtra::grid.arrange(grobs = purrr::map(1:36, mnist_plot), nrow = 6, top="Some MNIST examples")

```