---
format: html
params:
  sl_date: 2025-09-15
  fn: "Neural nets made ridiculously simple"
execute: 
  echo: false
  output: true
  freeze: false
categories: [AI/ML, beginner]
date: "`r params$sl_date`"
title: "`r params$fn`"
---

::: {.content-visible unless-format="revealjs"}

```{r}
#| echo: false
#| results: asis
#| fig-align: left
#| fig-height: 0.6
#| fig-width: 3

library(readr)
library(dplyr)
library(glue)
library(ggplot2)
library(sigmoid)

source(here::here("R/feed_block.R"))
feed_block("Neural Networks made ridiculously simple")
source(here::here("R/next_sesh.R"), local = T)
next_sesh(rmarkdown::metadata$params$fn)
```

:::

::: {.content-visible when-format="revealjs"}
```{r}
KINDR::training_sessions(tr_type = c("Skills", "AI/ML"), start_date = params$sl_date, n = 5, hide_area = T)
```
:::



## Welcome
* this session is ðŸŒ¶: for beginners

## What's this session for?

-   neural nets are a core technology for AI/ML systems
-   they've been around for decades (and probably will go on for decades)
-   they're also particularly helpful for health & care folk as a way of understanding AI/ML tools in general

## What this session won't do

-   give a general introduction to AI/ML
-   explain how to build a neural net of your very own
-   discuss in any detail the (often formidable) maths of neural nets

## Biology: the neurone

![[WikiMedia](https://commons.wikimedia.org/wiki/Category:Camillo_Golgi#/media/File:Golgi_1885_Plate_XIII.JPG)](../src/images/clipboard-3856000381.png){height="550px"}

-   [biological neurones](https://en.wikipedia.org/wiki/Neuron):
    -   receive some input from an upstream neuron(s)
    -   process that input in some way
    -   generate some output(s) in response to that input, and pass it downstream

## Biology: activation

![https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Action_potential.svg/778px-Action_potential.svg.png](../src/images/clipboard-3431060444.png){height="550px"}

-   neurones respond to stimulii
    -   threshold-y
    -   approximately digital output (on/off)
    -   sometimes complex behaviour about inputs

## Biology: networks of neurones

![[wikimedia](https://commons.wikimedia.org/wiki/Category:Camillo_Golgi#/media/File:Camillo_Golgi's_image_of_a_dog%E2%80%99s_olfactory_bulb\_(detail_2).jpg)](../src/images/clipboard-2953597145.png){height="550px"}

-   neurones are usually found in networks
-   can produce complex and sophisticated behaviours in a robust way </br> ![[Leon Benjamin on Flickr](https://www.flickr.com/photos/lbphotos/128557218)](../src/images/clipboard-2825173395.png)
-   non-obvious relationships between structure and function

## Machines: the node

Here's a simple representation of a node, implemented in code, that we might find in a neural network:

![](../src/images/net1.png)

## Machines: activation functions

Here are some example input:output pairs for our node:

![](../src/images/net2.png)

-   there are lots of possible activation functions
-   a simple one: `NOT`
    -   our node outputs TRUE when we input FALSE, and vice versa

This flexibility means that we can build networks of nodes (hence neural networks). Again, a very simple example:

## Activation functions can be extremely simple

```{r}
#| echo: true

node <- function(input){
  !input
}

node(TRUE)
```

## Machines: networks of nodes

![](../src/images/net3.png)

## Machines: networks of nodes

-   nodes are usually found in networks
-   can produce complex and sophisticated behaviours in a robust way </br> ![Chat GPT nonsense](../src/images/clipboard-1429075449.png){height="400px"}
-   again, non-obvious relationships between structure and function in artifical neural networks (ANN)


::: {.notes}
A user supplies some input. That input is fed into an input node(s), which processes the input, and produces three different outputs that are then fed into a second layer of nodes. Further processing happens in this hidden layer, leading to three outputs that are integrated together in a final output node that processes the outputs of the hidden layer into a single output.
:::


## Several kinds of networks

+ [there are lots of ways that neural networks can be arranged](https://www.asimovinstitute.org/neural-network-zoo/)
+ our example above = feed-forward
  + all the nodes are connected from left-to-right
+ but more complex architectures - like [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) - might have feedback looks and other biological-ish features
+ different numbers of layers
+ lots of different design tendencies since the first intro of neural nets in the 1950s [@rosenblatt1958]
+ most fancy ANNs are currently architecturally simple

## Why ANNs?
+ ANNs can can potentially replicate *any* input-output ransformation
+ we do that by a) increasing complexity and b) allowing them to 'learn'

. . .

![](../src/images/net4.png)


## Different activation functions

+ binary (true/false)
+ continuous
  + linear
  + non-linear (like sigmoid, ReLU)
+ different activation = different node behaviour = different network functions

## Sigmoid activation

```{r}
ggplot() +
  xlim(c(-15, 15)) +
  geom_function(fun = sigmoid) +
  theme_minimal() +
  ggtitle("Sigmoid")
```

## ReLU activation

```{r}
ggplot() +
  xlim(c(-15, 15)) +
  geom_function(fun = relu) +
  theme_minimal() +
  ggtitle("ReLU")
```

## Training in neural networks

+ ANNs can be trained
  + take a dataset 
  + split it into training and test parts
    + classify (by hand) the training data
  + then train
    + feed your ANN the training data and evaluate how well it performs
    + modify the ANN based on that evaluation
    + repeat until done/bored/perfect
  + finally, test your model with your unlabelled test data and evaluate
  
## MNIST

+ a [classic dataset](https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png) </br>![](../src/images/MnistExamplesModified.png)
+ recognizing handwritten numbers = actually-important task
+ 60000 labelled training images
+ 10000 test images
+ each is a 28\*28 pixel matrix grey vales encoded as 0-255

## MNIST data example

```{r}
mnist <- readRDS("data/mnist_sm.rds")
t(matrix(mnist$train$images[1,], 28, 28)) |>
  as_tibble() |>
  select(V10:V20) |>
  slice(5:10) |>
  knitr::kable()
```

## Train for MNIST

+ take your training data
+ put together a neural network (number of nodes, layers, feedback, activation functions)
+ run the training data, and evaluate based on labelling
+ modify your neural network, rinse, and repeat
+ ...
+ when happy, try the unlabelled test data

## MNIST examples

+ [lots of different examples](https://www.google.com/search?sca_esv=2570ae1cf58fa5c0&sca_upv=1&sxsrf=ADLYWIKxXMZkhLseWyic66zzdMajQ_FY3Q:1723463086461&q=mnist+neural+network&tbm=isch&source=lnms&fbs=AEQNm0Aa4sjWe7Rqy32pFwRj0UkWERaHdBms7t-tHL1116ec0FnDIxrxgGhNFSZEtYqV91QqM6LWLrLFWKmjC_P6yIDKAfdUXLK5TdVXK9XKaLJ0CvIevHNUoTv_L_edtCkFO49GwFOyIj9HDXPzAaIsMKrRU9-We_G08qTzCW2H2tx2SXc4OexQzDRMByEUFhbW4wpPcER3&sa=X&ved=2ahUKEwiP-LSosO-HAxWpT0EAHdVLOBsQ0pQJegQIERAB&biw=1842&bih=1007&dpr=1)
+ why do this work in an ANN, rather than in some other tool?
  + generic training strategy - reduces need for domain knowledge
  + hopefully robust outcomes - so giving models able to work across contexts
  + scalable

## MNIST in R

An aside here for the R enthusiasts - we can plot the handwritten numbers back out of the data using `ggplot()`:

```{r}
#| warning: false
#| message: false
#| cache: true
#| echo: true
#| output-location: slide

mnist_plot_dat <- function(df) {
   # matrix to pivoted tibble for plotting
  df |>
      as_tibble() |>
      mutate(rn = row_number()) |>
      tidyr::pivot_longer(!rn) |>
      mutate(name = as.numeric(gsub("V", "", name)))
}

mnist_main_plot <- function(df) {
  df |>
    ggplot() +
    geom_tile(aes(
      x = rn,
      y = reorder(name,-name),
      fill = value
    )) +
    scale_fill_gradient2(mid = "white", high = "black")
}

mnist_plot <- function(n){

  mnist_plot_dat(matrix(mnist$train$images[n, ], 28, 28)) |>
    mnist_main_plot() +
      ggtitle(glue("Label: { mnist$train$labels[n]}")) +
      theme_void() +
      theme(legend.position = "none")

}

gridExtra::grid.arrange(grobs = purrr::map(1:36, mnist_plot), nrow = 6, top="Some MNIST examples")

```

::: {.content-visible when-format="revealjs"}

## Forthcoming sessions

```{r}
KINDR::training_sessions(tr_type = c("Skills", "AI/ML"), start_date = params$sl_date, n = 5, hide_area = T)
```

## Feedback

Please can you [spare us one minute of your time to give feedback](`r KINDR::feedback_url(params$fn, date = params$sl_date)`)?


:::

